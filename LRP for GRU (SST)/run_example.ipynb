{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy import newaxis as na\n",
    "import codecs\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_score_by_abs (score, max_score, min_score):\n",
    "    # CASE 1: positive AND negative scores occur --------------------\n",
    "    if max_score>0 and min_score<0:\n",
    "    \n",
    "        if max_score >= abs(min_score):   # deepest color is positive\n",
    "            if score>=0:\n",
    "                return 0.5 + 0.5*(score/max_score)\n",
    "            else:\n",
    "                return 0.5 - 0.5*(abs(score)/max_score)\n",
    "\n",
    "        else:                             # deepest color is negative\n",
    "            if score>=0:\n",
    "                return 0.5 + 0.5*(score/abs(min_score))\n",
    "            else:\n",
    "                return 0.5 - 0.5*(score/min_score)   \n",
    "    \n",
    "    # CASE 2: ONLY positive scores occur -----------------------------       \n",
    "    elif max_score>0 and min_score>=0: \n",
    "        if max_score == min_score:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.5 + 0.5*(score/max_score)\n",
    "    \n",
    "    # CASE 3: ONLY negative scores occur -----------------------------\n",
    "    elif max_score<=0 and min_score<0: \n",
    "        if max_score == min_score:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 0.5 - 0.5*(score/min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRGB (c_tuple):\n",
    "    return \"#%02x%02x%02x\"%(int(c_tuple[0]*255), int(c_tuple[1]*255), int(c_tuple[2]*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_word (word, score, colormap):\n",
    "    return \"<span style=\\\"background-color:\"+getRGB(colormap(score))+\"\\\">\"+word+\"</span>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_heatmap (words, scores, cmap_name=\"bwr\"):\n",
    "    colormap  = plt.get_cmap(cmap_name)\n",
    "     \n",
    "    assert len(words)==len(scores)\n",
    "    max_s     = max(scores)\n",
    "    min_s     = min(scores)\n",
    "    \n",
    "    output_text = \"\"\n",
    "    \n",
    "    for idx, w in enumerate(words):\n",
    "        score       = rescale_score_by_abs(scores[idx], max_s, min_s)\n",
    "        output_text = output_text + span_word(w, score, colormap) + \" \"\n",
    "    \n",
    "    return output_text + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor=0.0, debug=False):\n",
    "    sign_out = np.where(hout[na,:]>=0, 1., -1.) # shape (1, M)\n",
    "    \n",
    "    numer    = (w * hin[:,na]) + ( bias_factor * (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units ) # shape (D, M)\n",
    "    # Note: here we multiply the bias_factor with both the bias b and the stabilizer eps since in fact\n",
    "    # using the term (b[na,:]*1. + eps*sign_out*1.) / bias_nb_units in the numerator is only useful for sanity check\n",
    "    # (in the initial paper version we were using (bias_factor*b[na,:]*1. + eps*sign_out*1.) / bias_nb_units instead)\n",
    "    \n",
    "    denom    = hout[na,:] + (eps*sign_out*1.)   # shape (1, M)\n",
    "    \n",
    "    message  = (numer/denom) * Rout[na,:]       # shape (D, M)\n",
    "    \n",
    "    Rin      = message.sum(axis=1)              # shape (D,)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"local diff: \", Rout.sum() - Rin.sum())\n",
    "    # Note: \n",
    "    # - local  layer   relevance conservation if bias_factor==1.0 and bias_nb_units==D (i.e. when only one incoming layer)\n",
    "    # - global network relevance conservation if bias_factor==1.0 and bias_nb_units set accordingly to the total number of lower-layer connections \n",
    "    # -> can be used for sanity check\n",
    "    \n",
    "    return Rin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_bidi:\n",
    "    \n",
    "    def __init__(self):\n",
    "   \n",
    "        # vocabulary\n",
    "        f_voc     = open(\"vocab\", 'rb')\n",
    "        self.voc  = pickle.load(f_voc)\n",
    "        f_voc.close()\n",
    "        \n",
    "        # word embeddings\n",
    "        self.E    = np.load('embeddings.npy', mmap_mode='r') # shape V*e\n",
    "        \n",
    "        # model weights\n",
    "        f_model   = open('model', 'rb')\n",
    "        model     = pickle.load(f_model)\n",
    "        f_model.close()\n",
    "        # LSTM left encoder\n",
    "        self.Wxh_Left  = model[\"Wxh_Left\"]  # shape 4d*e\n",
    "        self.bxh_Left  = model[\"bxh_Left\"]  # shape 4d \n",
    "        self.Whh_Left  = model[\"Whh_Left\"]  # shape 4d*d\n",
    "        self.bhh_Left  = model[\"bhh_Left\"]  # shape 4d  \n",
    "        # LSTM right encoder\n",
    "        self.Wxh_Right = model[\"Wxh_Right\"]\n",
    "        self.bxh_Right = model[\"bxh_Right\"]\n",
    "        self.Whh_Right = model[\"Whh_Right\"]\n",
    "        self.bhh_Right = model[\"bhh_Right\"]   \n",
    "        # linear output layer\n",
    "        self.Why_Left  = model[\"Why_Left\"]  # shape C*d\n",
    "        self.Why_Right = model[\"Why_Right\"] # shape C*d\n",
    "    \n",
    "\n",
    "    def set_input(self, w, delete_pos=None):\n",
    "        T      = len(w)                         # sequence length\n",
    "        d      = int(self.Wxh_Left.shape[0]/4)  # hidden layer dimension\n",
    "        e      = self.E.shape[1]                # word embedding dimension\n",
    "        x      = np.zeros((T, e))\n",
    "        x[:,:] = self.E[w,:]\n",
    "        if delete_pos is not None:\n",
    "            x[delete_pos, :] = np.zeros((len(delete_pos), e))\n",
    "        \n",
    "        self.w              = w\n",
    "        self.x              = x\n",
    "        self.x_rev          = x[::-1,:].copy()\n",
    "        \n",
    "        self.h_Left         = np.zeros((T+1, d))\n",
    "        self.c_Left         = np.zeros((T+1, d))\n",
    "        self.h_Right        = np.zeros((T+1, d))\n",
    "        self.c_Right        = np.zeros((T+1, d))\n",
    "     \n",
    "   \n",
    "    def forward(self):\n",
    "\n",
    "        T      = len(self.w)                         \n",
    "        d      = int(self.Wxh_Left.shape[0]/4) \n",
    "        # gate indices (assuming the gate ordering in the LSTM weights is i,g,f,o):     \n",
    "        idx    = np.hstack((np.arange(0,d), np.arange(2*d,4*d))).astype(int) # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = np.arange(0,d), np.arange(d,2*d), np.arange(2*d,3*d), np.arange(3*d,4*d) # indices of gates i,g,f,o separately\n",
    "          \n",
    "        # initialize\n",
    "        self.gates_xh_Left  = np.zeros((T, 4*d))  \n",
    "        self.gates_hh_Left  = np.zeros((T, 4*d)) \n",
    "        self.gates_pre_Left = np.zeros((T, 4*d))  # gates pre-activation\n",
    "        self.gates_Left     = np.zeros((T, 4*d))  # gates activation\n",
    "        \n",
    "        self.gates_xh_Right = np.zeros((T, 4*d))  \n",
    "        self.gates_hh_Right = np.zeros((T, 4*d)) \n",
    "        self.gates_pre_Right= np.zeros((T, 4*d))\n",
    "        self.gates_Right    = np.zeros((T, 4*d)) \n",
    "             \n",
    "        for t in range(T): \n",
    "            self.gates_xh_Left[t]     = np.dot(self.Wxh_Left, self.x[t])        \n",
    "            self.gates_hh_Left[t]     = np.dot(self.Whh_Left, self.h_Left[t-1]) \n",
    "            self.gates_pre_Left[t]    = self.gates_xh_Left[t] + self.gates_hh_Left[t] + self.bxh_Left + self.bhh_Left\n",
    "            self.gates_Left[t,idx]    = 1.0/(1.0 + np.exp(- self.gates_pre_Left[t,idx]))\n",
    "            self.gates_Left[t,idx_g]  = np.tanh(self.gates_pre_Left[t,idx_g]) \n",
    "            self.c_Left[t]            = self.gates_Left[t,idx_f]*self.c_Left[t-1] + self.gates_Left[t,idx_i]*self.gates_Left[t,idx_g]\n",
    "            self.h_Left[t]            = self.gates_Left[t,idx_o]*np.tanh(self.c_Left[t])\n",
    "            \n",
    "            self.gates_xh_Right[t]    = np.dot(self.Wxh_Right, self.x_rev[t])     \n",
    "            self.gates_hh_Right[t]    = np.dot(self.Whh_Right, self.h_Right[t-1])\n",
    "            self.gates_pre_Right[t]   = self.gates_xh_Right[t] + self.gates_hh_Right[t] + self.bxh_Right + self.bhh_Right\n",
    "            self.gates_Right[t,idx]   = 1.0/(1.0 + np.exp(- self.gates_pre_Right[t,idx]))\n",
    "            self.gates_Right[t,idx_g] = np.tanh(self.gates_pre_Right[t,idx_g])                 \n",
    "            self.c_Right[t]           = self.gates_Right[t,idx_f]*self.c_Right[t-1] + self.gates_Right[t,idx_i]*self.gates_Right[t,idx_g]\n",
    "            self.h_Right[t]           = self.gates_Right[t,idx_o]*np.tanh(self.c_Right[t])\n",
    "            \n",
    "        self.y_Left  = np.dot(self.Why_Left,  self.h_Left[T-1])\n",
    "        self.y_Right = np.dot(self.Why_Right, self.h_Right[T-1])\n",
    "        self.s       = self.y_Left + self.y_Right\n",
    "        \n",
    "        return self.s.copy() # prediction scores\n",
    "     \n",
    "              \n",
    "    def backward(self, w, sensitivity_class):\n",
    "\n",
    "        # forward pass\n",
    "        self.set_input(w)\n",
    "        self.forward() \n",
    "        \n",
    "        T      = len(self.w)\n",
    "        d      = int(self.Wxh_Left.shape[0]/4)\n",
    "        C      = self.Why_Left.shape[0]   # number of classes\n",
    "        idx    = np.hstack((np.arange(0,d), np.arange(2*d,4*d))).astype(int) # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = np.arange(0,d), np.arange(d,2*d), np.arange(2*d,3*d), np.arange(3*d,4*d) # indices of gates i,g,f,o separately\n",
    "        \n",
    "        # initialize\n",
    "        self.dx               = np.zeros(self.x.shape)\n",
    "        self.dx_rev           = np.zeros(self.x.shape)\n",
    "        \n",
    "        self.dh_Left          = np.zeros((T+1, d))\n",
    "        self.dc_Left          = np.zeros((T+1, d))\n",
    "        self.dgates_pre_Left  = np.zeros((T, 4*d))  # gates pre-activation\n",
    "        self.dgates_Left      = np.zeros((T, 4*d))  # gates activation\n",
    "        \n",
    "        self.dh_Right         = np.zeros((T+1, d))\n",
    "        self.dc_Right         = np.zeros((T+1, d))\n",
    "        self.dgates_pre_Right = np.zeros((T, 4*d)) \n",
    "        self.dgates_Right     = np.zeros((T, 4*d))  \n",
    "               \n",
    "        ds                    = np.zeros((C))\n",
    "        ds[sensitivity_class] = 1.0\n",
    "        dy_Left               = ds.copy()\n",
    "        dy_Right              = ds.copy()\n",
    "        \n",
    "        self.dh_Left[T-1]     = np.dot(self.Why_Left.T,  dy_Left)\n",
    "        self.dh_Right[T-1]    = np.dot(self.Why_Right.T, dy_Right)\n",
    "        \n",
    "        for t in reversed(range(T)): \n",
    "            self.dgates_Left[t,idx_o]    = self.dh_Left[t] * np.tanh(self.c_Left[t])  # do[t]\n",
    "            self.dc_Left[t]             += self.dh_Left[t] * self.gates_Left[t,idx_o] * (1.-(np.tanh(self.c_Left[t]))**2) # dc[t]\n",
    "            self.dgates_Left[t,idx_f]    = self.dc_Left[t] * self.c_Left[t-1]         # df[t]\n",
    "            self.dc_Left[t-1]            = self.dc_Left[t] * self.gates_Left[t,idx_f] # dc[t-1]\n",
    "            self.dgates_Left[t,idx_i]    = self.dc_Left[t] * self.gates_Left[t,idx_g] # di[t]\n",
    "            self.dgates_Left[t,idx_g]    = self.dc_Left[t] * self.gates_Left[t,idx_i] # dg[t]\n",
    "            self.dgates_pre_Left[t,idx]  = self.dgates_Left[t,idx] * self.gates_Left[t,idx] * (1.0 - self.gates_Left[t,idx]) # d ifo pre[t]\n",
    "            self.dgates_pre_Left[t,idx_g]= self.dgates_Left[t,idx_g] *  (1.-(self.gates_Left[t,idx_g])**2) # d g pre[t]\n",
    "            self.dh_Left[t-1]            = np.dot(self.Whh_Left.T, self.dgates_pre_Left[t])\n",
    "            self.dx[t]                   = np.dot(self.Wxh_Left.T, self.dgates_pre_Left[t])\n",
    "            \n",
    "            self.dgates_Right[t,idx_o]    = self.dh_Right[t] * np.tanh(self.c_Right[t])         \n",
    "            self.dc_Right[t]             += self.dh_Right[t] * self.gates_Right[t,idx_o] * (1.-(np.tanh(self.c_Right[t]))**2) \n",
    "            self.dgates_Right[t,idx_f]    = self.dc_Right[t] * self.c_Right[t-1]            \n",
    "            self.dc_Right[t-1]            = self.dc_Right[t] * self.gates_Right[t,idx_f] \n",
    "            self.dgates_Right[t,idx_i]    = self.dc_Right[t] * self.gates_Right[t,idx_g]    \n",
    "            self.dgates_Right[t,idx_g]    = self.dc_Right[t] * self.gates_Right[t,idx_i]      \n",
    "            self.dgates_pre_Right[t,idx]  = self.dgates_Right[t,idx] * self.gates_Right[t,idx] * (1.0 - self.gates_Right[t,idx]) \n",
    "            self.dgates_pre_Right[t,idx_g]= self.dgates_Right[t,idx_g] *  (1.-(self.gates_Right[t,idx_g])**2) \n",
    "            self.dh_Right[t-1]            = np.dot(self.Whh_Right.T, self.dgates_pre_Right[t])\n",
    "            self.dx_rev[t]                = np.dot(self.Wxh_Right.T, self.dgates_pre_Right[t])\n",
    "                    \n",
    "        return self.dx.copy(), self.dx_rev[::-1,:].copy()     \n",
    "    \n",
    "                   \n",
    "    def lrp(self, w, LRP_class, eps=0.001, bias_factor=0.0):\n",
    "\n",
    "        # forward pass\n",
    "        self.set_input(w)\n",
    "        self.forward() \n",
    "        \n",
    "        T      = len(self.w)\n",
    "        d      = int(self.Wxh_Left.shape[0]/4)\n",
    "        e      = self.E.shape[1] \n",
    "        C      = self.Why_Left.shape[0]  # number of classes\n",
    "        idx    = np.hstack((np.arange(0,d), np.arange(2*d,4*d))).astype(int) # indices of gates i,f,o together\n",
    "        idx_i, idx_g, idx_f, idx_o = np.arange(0,d), np.arange(d,2*d), np.arange(2*d,3*d), np.arange(3*d,4*d) # indices of gates i,g,f,o separately\n",
    "        \n",
    "        # initialize\n",
    "        Rx       = np.zeros(self.x.shape)\n",
    "        Rx_rev   = np.zeros(self.x.shape)\n",
    "        \n",
    "        Rh_Left  = np.zeros((T+1, d))\n",
    "        Rc_Left  = np.zeros((T+1, d))\n",
    "        Rg_Left  = np.zeros((T,   d)) # gate g only\n",
    "        Rh_Right = np.zeros((T+1, d))\n",
    "        Rc_Right = np.zeros((T+1, d))\n",
    "        Rg_Right = np.zeros((T,   d)) # gate g only\n",
    "        \n",
    "        Rout_mask            = np.zeros((C))\n",
    "        Rout_mask[LRP_class] = 1.0  \n",
    "        \n",
    "        # format reminder: lrp_linear(hin, w, b, hout, Rout, bias_nb_units, eps, bias_factor)\n",
    "        Rh_Left[T-1]  = lrp_linear(self.h_Left[T-1],  self.Why_Left.T , np.zeros((C)), self.s, self.s*Rout_mask, 2*d, eps, bias_factor, debug=False)\n",
    "        Rh_Right[T-1] = lrp_linear(self.h_Right[T-1], self.Why_Right.T, np.zeros((C)), self.s, self.s*Rout_mask, 2*d, eps, bias_factor, debug=False)\n",
    "        \n",
    "        for t in reversed(range(T)):\n",
    "            Rc_Left[t]   += Rh_Left[t]\n",
    "            Rc_Left[t-1]  = lrp_linear(self.gates_Left[t,idx_f]*self.c_Left[t-1],         np.identity(d), np.zeros((d)), self.c_Left[t], Rc_Left[t], 2*d, eps, bias_factor, debug=False)\n",
    "            Rg_Left[t]    = lrp_linear(self.gates_Left[t,idx_i]*self.gates_Left[t,idx_g], np.identity(d), np.zeros((d)), self.c_Left[t], Rc_Left[t], 2*d, eps, bias_factor, debug=False)\n",
    "            Rx[t]         = lrp_linear(self.x[t],        self.Wxh_Left[idx_g].T, self.bxh_Left[idx_g]+self.bhh_Left[idx_g], self.gates_pre_Left[t,idx_g], Rg_Left[t], d+e, eps, bias_factor, debug=False)\n",
    "            Rh_Left[t-1]  = lrp_linear(self.h_Left[t-1], self.Whh_Left[idx_g].T, self.bxh_Left[idx_g]+self.bhh_Left[idx_g], self.gates_pre_Left[t,idx_g], Rg_Left[t], d+e, eps, bias_factor, debug=False)\n",
    "            \n",
    "            Rc_Right[t]  += Rh_Right[t]\n",
    "            Rc_Right[t-1] = lrp_linear(self.gates_Right[t,idx_f]*self.c_Right[t-1],         np.identity(d), np.zeros((d)), self.c_Right[t], Rc_Right[t], 2*d, eps, bias_factor, debug=False)\n",
    "            Rg_Right[t]   = lrp_linear(self.gates_Right[t,idx_i]*self.gates_Right[t,idx_g], np.identity(d), np.zeros((d)), self.c_Right[t], Rc_Right[t], 2*d, eps, bias_factor, debug=False)\n",
    "            Rx_rev[t]     = lrp_linear(self.x_rev[t],     self.Wxh_Right[idx_g].T, self.bxh_Right[idx_g]+self.bhh_Right[idx_g], self.gates_pre_Right[t,idx_g], Rg_Right[t], d+e, eps, bias_factor, debug=False)\n",
    "            Rh_Right[t-1] = lrp_linear(self.h_Right[t-1], self.Whh_Right[idx_g].T, self.bxh_Right[idx_g]+self.bhh_Right[idx_g], self.gates_pre_Right[t,idx_g], Rg_Right[t], d+e, eps, bias_factor, debug=False)\n",
    "                   \n",
    "        return Rx, Rx_rev[::-1,:], Rh_Left[-1].sum()+Rc_Left[-1].sum()+Rh_Right[-1].sum()+Rc_Right[-1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(words):\n",
    "    net                 = LSTM_bidi()                                   # load trained LSTM model\n",
    "    w_indices           = [net.voc.index(w) for w in words]             # convert input sentence to word IDs\n",
    "    net.set_input(w_indices)                                            # set LSTM input sequence\n",
    "    scores              = net.forward()                                 # classification prediction scores\n",
    "    return np.argmax(scores)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, uncomment one of the following sentences, or define your own sequence (only words contained in the vocabulary are supported!)\n",
    "#words = ['this','movie','was','actually','neither','that','funny',',','nor','super','witty','.']\n",
    "#words = ['this', 'film', 'does', 'n\\'t', 'care', 'about', 'cleverness', ',', 'wit', 'or', 'any', 'other', 'kind', 'of', 'intelligent', 'humor', '.']\n",
    "words = ['i','hate','the','movie','though','the','plot','is','interesting','.']\n",
    "#words = ['used', 'to', 'be', 'my', 'favorite']\n",
    "#words = ['not', 'worth', 'the', 'time']\n",
    "#words = ['is', 'n\\'t', 'a', 'bad', 'film'] # Note: misclassified sample!\n",
    "#words = ['is', 'n\\'t', 'very', 'interesting'] \n",
    "#words = ['it', '\\'s', 'easy' ,'to' ,'love' ,'robin' ,'tunney' ,'--' ,'she' ,'\\'s' ,'pretty' ,'and' ,'she' ,'can' ,'act' ,'--' ,'but' ,'it' ,'gets' ,'harder' ,'and' ,'harder' ,'to' ,'understand' ,'her' ,'choices', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = predict(words)                                        # get predicted class\n",
    "target_class    = predicted_class                                       # define relevance target class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hate', 'the', 'movie', 'though', 'the', 'plot', 'is', 'interesting', '.']\n",
      "\n",
      "predicted class:           0\n"
     ]
    }
   ],
   "source": [
    "print (words)\n",
    "print (\"\\npredicted class:          \",   predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRP hyperparameters:\n",
    "eps                 = 0.001                                             # small positive number\n",
    "bias_factor         = 0.0                                               # recommended value\n",
    " \n",
    "net                 = LSTM_bidi()                                       # load trained LSTM model\n",
    "\n",
    "w_indices           = [net.voc.index(w) for w in words]                 # convert input sentence to word IDs\n",
    "Rx, Rx_rev, R_rest  = net.lrp(w_indices, target_class, eps, bias_factor)# perform LRP\n",
    "R_words             = np.sum(Rx + Rx_rev, axis=1)                       # compute word-level LRP relevances\n",
    "\n",
    "scores              = net.s.copy()                                      # classification prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction scores:         [ 0.83815775  0.55615975 -0.23874953  0.06664621 -0.48527585]\n",
      "\n",
      "LRP target class:          0\n",
      "\n",
      "LRP relevances:\n",
      "\t\t\t   -0.07\ti\n",
      "\t\t\t    2.78\thate\n",
      "\t\t\t   -0.06\tthe\n",
      "\t\t\t    0.23\tmovie\n",
      "\t\t\t    0.75\tthough\n",
      "\t\t\t   -0.01\tthe\n",
      "\t\t\t    2.10\tplot\n",
      "\t\t\t   -1.16\tis\n",
      "\t\t\t   -6.09\tinteresting\n",
      "\t\t\t   -0.51\t.\n",
      "\n",
      "LRP heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fcfcff\">i</span> <span style=\"background-color:#ff8a8a\">hate</span> <span style=\"background-color:#fcfcff\">the</span> <span style=\"background-color:#fff6f6\">movie</span> <span style=\"background-color:#ffe0e0\">though</span> <span style=\"background-color:#fefeff\">the</span> <span style=\"background-color:#ffa6a6\">plot</span> <span style=\"background-color:#ceceff\">is</span> <span style=\"background-color:#0000ff\">interesting</span> <span style=\"background-color:#eaeaff\">.</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"prediction scores:        \",   scores)\n",
    "print (\"\\nLRP target class:         \", target_class)\n",
    "print (\"\\nLRP relevances:\")\n",
    "for idx, w in enumerate(words):\n",
    "    print (\"\\t\\t\\t\" + \"{:8.2f}\".format(R_words[idx]) + \"\\t\" + w)\n",
    "print (\"\\nLRP heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8381577526347607\n",
      "Sanity check passed?  True\n"
     ]
    }
   ],
   "source": [
    "# How to sanity check global relevance conservation:\n",
    "bias_factor        = 1.0                                             # value to use for sanity check\n",
    "Rx, Rx_rev, R_rest = net.lrp(w_indices, target_class, eps, bias_factor)\n",
    "R_tot              = Rx.sum() + Rx_rev.sum() + R_rest.sum()          # sum of all \"input\" relevances\n",
    "\n",
    "print(R_tot)       ;    print(\"Sanity check passed? \", np.allclose(R_tot, net.s[target_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net              = LSTM_bidi()                                       # load trained LSTM model\n",
    "\n",
    "w_indices        = [net.voc.index(w) for w in words]                 # convert input sentence to word IDs\n",
    "Gx, Gx_rev       = net.backward(w_indices, target_class)             # perform gradient backpropagation\n",
    "R_words_SA       = (np.linalg.norm(Gx + Gx_rev, ord=2, axis=1))**2   # compute word-level Sensitivity Analysis relevances\n",
    "R_words_GI       = ((Gx + Gx_rev)*net.x).sum(axis=1)                 # compute word-level GradientxInput relevances\n",
    "\n",
    "scores           = net.s.copy()                                      # classification prediction scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction scores:        [ 0.83815775  0.55615975 -0.23874953  0.06664621 -0.48527585]\n",
      "\n",
      "SA/GI target class:       0\n",
      "\n",
      "SA relevances:\n",
      "\t\t\t    0.92\ti\n",
      "\t\t\t   50.03\thate\n",
      "\t\t\t    0.57\tthe\n",
      "\t\t\t    6.77\tmovie\n",
      "\t\t\t   14.29\tthough\n",
      "\t\t\t    0.60\tthe\n",
      "\t\t\t   23.59\tplot\n",
      "\t\t\t   11.33\tis\n",
      "\t\t\t   35.48\tinteresting\n",
      "\t\t\t    1.80\t.\n",
      "\n",
      "SA heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fffafa\">i</span> <span style=\"background-color:#ff0000\">hate</span> <span style=\"background-color:#fffcfc\">the</span> <span style=\"background-color:#ffdcdc\">movie</span> <span style=\"background-color:#ffb6b6\">though</span> <span style=\"background-color:#fffcfc\">the</span> <span style=\"background-color:#ff8686\">plot</span> <span style=\"background-color:#ffc6c6\">is</span> <span style=\"background-color:#ff4949\">interesting</span> <span style=\"background-color:#fff6f6\">.</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GI relevances:\n",
      "\t\t\t   -0.05\ti\n",
      "\t\t\t    2.09\thate\n",
      "\t\t\t    0.47\tthe\n",
      "\t\t\t   -0.12\tmovie\n",
      "\t\t\t    0.50\tthough\n",
      "\t\t\t    0.58\tthe\n",
      "\t\t\t    0.98\tplot\n",
      "\t\t\t    0.54\tis\n",
      "\t\t\t   -2.68\tinteresting\n",
      "\t\t\t    0.43\t.\n",
      "\n",
      "GI heatmap:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fafaff\">i</span> <span style=\"background-color:#ff3838\">hate</span> <span style=\"background-color:#ffd2d2\">the</span> <span style=\"background-color:#f3f3ff\">movie</span> <span style=\"background-color:#ffcece\">though</span> <span style=\"background-color:#ffc8c8\">the</span> <span style=\"background-color:#ffa2a2\">plot</span> <span style=\"background-color:#ffcccc\">is</span> <span style=\"background-color:#0000ff\">interesting</span> <span style=\"background-color:#ffd6d6\">.</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"prediction scores:       \",   scores)\n",
    "print (\"\\nSA/GI target class:      \", target_class)\n",
    "print (\"\\nSA relevances:\")\n",
    "for idx, w in enumerate(words):\n",
    "    print (\"\\t\\t\\t\" + \"{:8.2f}\".format(R_words_SA[idx]) + \"\\t\" + w)\n",
    "print (\"\\nSA heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words_SA)))\n",
    "print (\"\\nGI relevances:\")\n",
    "for idx, w in enumerate(words):\n",
    "    print (\"\\t\\t\\t\" + \"{:8.2f}\".format(R_words_GI[idx]) + \"\\t\" + w)\n",
    "print (\"\\nGI heatmap:\")    \n",
    "display(HTML(html_heatmap(words, R_words_GI)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Recursive Neural Tensor Network\n",
    "# prediction scores:  [ 8.90618621e-01  1.01122260e+00  2.26404237e-04 -3.12216870e-01 -1.37174135e+00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
