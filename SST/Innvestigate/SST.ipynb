{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c815835-4e5a-4540-9cfe-92a5dcd7236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import imp\n",
    "import keras.backend\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "from matplotlib import cm, transforms\n",
    "\n",
    "import innvestigate\n",
    "import innvestigate.applications\n",
    "import innvestigate.applications.mnist\n",
    "import innvestigate.utils as iutils\n",
    "import innvestigate.utils.visualizations as ivis\n",
    "from innvestigate.utils.tests.networks import base as network_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e765430-2741-4d59-8ff3-d1699860f0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  stanfordSentimentTreebank.zip\n",
      "   creating: stanfordSentimentTreebank/\n",
      "  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/stanfordSentimentTreebank/\n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n",
      "  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n",
      "  inflating: stanfordSentimentTreebank/dictionary.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n",
      "  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n",
      "  inflating: stanfordSentimentTreebank/README.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n",
      "  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n",
      "  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n",
      "  inflating: stanfordSentimentTreebank/SOStr.txt  \n",
      "  inflating: stanfordSentimentTreebank/STree.txt  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0   329    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 6223k  100 6223k    0     0   704k      0  0:00:08  0:00:08 --:--:--  944k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -d \"./stanfordSentimentTreebank\" ]; then\n",
    "    curl -L http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip -O && unzip stanfordSentimentTreebank.zip\n",
    "else\n",
    "    echo \"The data is already there. Skip downloading!!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c2a6c7-de1d-488b-8c39-a9934bbbd98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   147  100   147    0     0    309      0 --:--:-- --:--:-- --:--:--   310\n",
      "100 9158k  100 9158k    0     0  4722k      0  0:00:01  0:00:01 --:--:-- 7338k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   138  100   138    0     0    525      0 --:--:-- --:--:-- --:--:--   526\n",
      "100  207k  100  207k    0     0   348k      0 --:--:-- --:--:-- --:--:--  348k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -e \"./stanfordSentimentTreebank/embeddings.npy\" ]; then\n",
    "    curl -L https://github.com/ArrasL/LRP_for_LSTM/raw/master/model/embeddings.npy -o stanfordSentimentTreebank/embeddings.npy &&\n",
    "        curl -L https://github.com/ArrasL/LRP_for_LSTM/raw/master/model/vocab -o stanfordSentimentTreebank/vocab\n",
    "else\n",
    "    echo \"The data is already there. Skip downloading!!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084812b6-1a46-4afc-94ce-b0904d34b9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ftfy in c:\\users\\faraz\\appdata\\roaming\\python\\python36\\site-packages (6.0.3)\n",
      "Requirement already satisfied: wcwidth in c:\\program files\\python36\\lib\\site-packages (from ftfy) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29fdd7b7-7c6c-429a-ae6c-c84a44ddfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftfy import fix_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c3a867-f418-4948-ad48-bf5158749f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './stanfordSentimentTreebank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee45375-8435-4775-af4b-7f1f23b72fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 19539 vocabs.\n"
     ]
    }
   ],
   "source": [
    "with open('%s/vocab' % DATA_PATH, 'rb') as f:\n",
    "    vocabs = pickle.load(f) \n",
    "    total_vocabs = len(vocabs) \n",
    "\n",
    "    # Unknown vocabs are set to <UNK>.\n",
    "    encoder = dict(zip(['<UNK>'] + vocabs, range(0, len(vocabs) +1)))\n",
    "    decoder = dict(zip(encoder.values(), encoder.keys()))\n",
    "    \n",
    "    print('We have %d vocabs.' % len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91fde83-2136-428b-8642-2d992b8f48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding = np.load('%s/embeddings.npy' % DATA_PATH)\n",
    "\n",
    "# Unknown vocabs will have embedding weights of zero.\n",
    "embedding = np.zeros((pretrained_embedding.shape[0]+1, pretrained_embedding.shape[1]))\n",
    "embedding[1:, :] = pretrained_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87708e4-db5b-48b8-b399-63490e9fff9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phase</th>\n",
       "      <th>sostr</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>phase_id</th>\n",
       "      <th>sentiment_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The|Rock|is|destined|to|be|the|21st|Century|'s...</td>\n",
       "      <td>1</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The|gorgeously|elaborate|continuation|of|``|Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>Effective|but|too-tepid|biopic</td>\n",
       "      <td>2</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.51389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If|you|sometimes|like|to|go|to|the|movies|to|h...</td>\n",
       "      <td>2</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.73611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges|as|something|rare|,|an|issue|movie|tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.86111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence  \\\n",
       "0               1  The Rock is destined to be the 21st Century 's...   \n",
       "1               2  The gorgeously elaborate continuation of `` Th...   \n",
       "2               3                     Effective but too-tepid biopic   \n",
       "3               4  If you sometimes like to go to the movies to h...   \n",
       "4               5  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "                                               phase  \\\n",
       "0  The Rock is destined to be the 21st Century 's...   \n",
       "1  The gorgeously elaborate continuation of `` Th...   \n",
       "2                     Effective but too-tepid biopic   \n",
       "3  If you sometimes like to go to the movies to h...   \n",
       "4  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "                                               sostr  splitset_label  \\\n",
       "0  The|Rock|is|destined|to|be|the|21st|Century|'s...               1   \n",
       "1  The|gorgeously|elaborate|continuation|of|``|Th...               1   \n",
       "2                     Effective|but|too-tepid|biopic               2   \n",
       "3  If|you|sometimes|like|to|go|to|the|movies|to|h...               2   \n",
       "4  Emerges|as|something|rare|,|an|issue|movie|tha...               2   \n",
       "\n",
       "   phase_id  sentiment_value  \n",
       "0    226166          0.69444  \n",
       "1    226300          0.83333  \n",
       "2     13995          0.51389  \n",
       "3     14123          0.73611  \n",
       "4     13999          0.86111  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all necessary files\n",
    "df_reviews = pd.read_csv('%s/datasetSentences.txt' % DATA_PATH, sep='\\t')\n",
    "\n",
    "df_reviews['phase'] = df_reviews.sentence.apply(lambda s: fix_encoding(s))\\\n",
    "    .apply(lambda s: s.replace('-LRB-', '(').replace('-RRB-', ')'))\n",
    "\n",
    "df_reviews['sostr'] = pd.read_csv('%s/SOStr.txt' % DATA_PATH,\n",
    "                                  sep='\\t',encoding='utf-8',\n",
    "                                  header=None, names=['sostr']\n",
    "                                 )\n",
    "\n",
    "df_reviews['splitset_label'] = pd.read_csv('%s/datasetSplit.txt' % DATA_PATH,\n",
    "                                           sep=',', header=0\n",
    "                                          )['splitset_label']\n",
    "\n",
    "\n",
    "df_phases = pd.read_csv('%s/dictionary.txt' % DATA_PATH,\n",
    "                        sep='|', names=['phase', 'phase_id']\n",
    "                       )\n",
    "\n",
    "df_sentiment_labels = pd.read_csv('%s/sentiment_labels.txt' % DATA_PATH,\n",
    "                                  sep='|', names=['phase_id', 'sentiment_value'],\n",
    "                                  header=0\n",
    "                                 )\n",
    "\n",
    "df_reviews_with_sentiment_value = df_reviews.merge(df_phases, how='inner', on=['phase'])\\\n",
    "    .merge(df_sentiment_labels, on='phase_id')\n",
    "\n",
    "df_reviews_with_sentiment_value[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "664c75e6-6e96-449f-80ba-d1a7acd19ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_discretizer(sentiment_value):\n",
    "    if 0 <= sentiment_value <= 0.2:\n",
    "        return 'very_negative'\n",
    "    elif 0.2 < sentiment_value <= 0.4:\n",
    "        return 'negative'\n",
    "    elif 0.4 < sentiment_value <= 0.6:\n",
    "        return 'neutral'\n",
    "    elif 0.6 < sentiment_value <= 0.8:\n",
    "        return 'positive'\n",
    "    elif 0.8 < sentiment_value <= 1:\n",
    "        return 'very_positive'\n",
    "    \n",
    "df_reviews_with_sentiment_value['label'] = df_reviews_with_sentiment_value.sentiment_value.apply(sentiment_discretizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0812405-9675-4f70-b033-eba88b32838e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phase</th>\n",
       "      <th>sostr</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>phase_id</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The|Rock|is|destined|to|be|the|21st|Century|'s...</td>\n",
       "      <td>1</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The|gorgeously|elaborate|continuation|of|``|Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>Effective|but|too-tepid|biopic</td>\n",
       "      <td>2</td>\n",
       "      <td>13995</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If|you|sometimes|like|to|go|to|the|movies|to|h...</td>\n",
       "      <td>2</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.73611</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges|as|something|rare|,|an|issue|movie|tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.86111</td>\n",
       "      <td>very_positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence  \\\n",
       "0               1  The Rock is destined to be the 21st Century 's...   \n",
       "1               2  The gorgeously elaborate continuation of `` Th...   \n",
       "2               3                     Effective but too-tepid biopic   \n",
       "3               4  If you sometimes like to go to the movies to h...   \n",
       "4               5  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "                                               phase  \\\n",
       "0  The Rock is destined to be the 21st Century 's...   \n",
       "1  The gorgeously elaborate continuation of `` Th...   \n",
       "2                     Effective but too-tepid biopic   \n",
       "3  If you sometimes like to go to the movies to h...   \n",
       "4  Emerges as something rare , an issue movie tha...   \n",
       "\n",
       "                                               sostr  splitset_label  \\\n",
       "0  The|Rock|is|destined|to|be|the|21st|Century|'s...               1   \n",
       "1  The|gorgeously|elaborate|continuation|of|``|Th...               1   \n",
       "2                     Effective|but|too-tepid|biopic               2   \n",
       "3  If|you|sometimes|like|to|go|to|the|movies|to|h...               2   \n",
       "4  Emerges|as|something|rare|,|an|issue|movie|tha...               2   \n",
       "\n",
       "   phase_id  sentiment_value          label  \n",
       "0    226166          0.69444       positive  \n",
       "1    226300          0.83333  very_positive  \n",
       "2     13995          0.51389        neutral  \n",
       "3     14123          0.73611       positive  \n",
       "4     13999          0.86111  very_positive  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews_with_sentiment_value[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6750c529-5632-4a70-9514-8d126254ca3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11855"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_reviews_with_sentiment_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff6e602-2cb7-4478-bf06-e21df5aa6874",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAPPING = {\n",
    "    'very_negative': 0,\n",
    "    'negative': 0,\n",
    "    'positive': 1, \n",
    "    'very_positive': 1\n",
    "}\n",
    "\n",
    "LABEL_IDX_TO_NAME = {\n",
    "    0: 'negative',\n",
    "    1: 'positive'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e73a6e55-3a53-42d0-9fed-188fb8cf07e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 classes.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(LABEL_MAPPING.values()))\n",
    "print('We have %d classes.' % NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8caa47d0-8968-4356-a514-0f87e1bbaddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phase</th>\n",
       "      <th>sostr</th>\n",
       "      <th>splitset_label</th>\n",
       "      <th>phase_id</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>label</th>\n",
       "      <th>label_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>The|Rock|is|destined|to|be|the|21st|Century|'s...</td>\n",
       "      <td>1</td>\n",
       "      <td>226166</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>The|gorgeously|elaborate|continuation|of|``|Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>226300</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>If|you|sometimes|like|to|go|to|the|movies|to|h...</td>\n",
       "      <td>2</td>\n",
       "      <td>14123</td>\n",
       "      <td>0.73611</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>Emerges|as|something|rare|,|an|issue|movie|tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>13999</td>\n",
       "      <td>0.86111</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>Offers|that|rare|combination|of|entertainment|...</td>\n",
       "      <td>2</td>\n",
       "      <td>14351</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>very_positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence  \\\n",
       "0               1  The Rock is destined to be the 21st Century 's...   \n",
       "1               2  The gorgeously elaborate continuation of `` Th...   \n",
       "3               4  If you sometimes like to go to the movies to h...   \n",
       "4               5  Emerges as something rare , an issue movie tha...   \n",
       "6               7  Offers that rare combination of entertainment ...   \n",
       "\n",
       "                                               phase  \\\n",
       "0  The Rock is destined to be the 21st Century 's...   \n",
       "1  The gorgeously elaborate continuation of `` Th...   \n",
       "3  If you sometimes like to go to the movies to h...   \n",
       "4  Emerges as something rare , an issue movie tha...   \n",
       "6  Offers that rare combination of entertainment ...   \n",
       "\n",
       "                                               sostr  splitset_label  \\\n",
       "0  The|Rock|is|destined|to|be|the|21st|Century|'s...               1   \n",
       "1  The|gorgeously|elaborate|continuation|of|``|Th...               1   \n",
       "3  If|you|sometimes|like|to|go|to|the|movies|to|h...               2   \n",
       "4  Emerges|as|something|rare|,|an|issue|movie|tha...               2   \n",
       "6  Offers|that|rare|combination|of|entertainment|...               2   \n",
       "\n",
       "   phase_id  sentiment_value          label  label_idx  \n",
       "0    226166          0.69444       positive          1  \n",
       "1    226300          0.83333  very_positive          1  \n",
       "3     14123          0.73611       positive          1  \n",
       "4     13999          0.86111  very_positive          1  \n",
       "6     14351          0.83333  very_positive          1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_indices = df_reviews_with_sentiment_value.label.apply(lambda l: l in LABEL_MAPPING)\n",
    "\n",
    "df_reviews_with_sentiment_value_filtered = df_reviews_with_sentiment_value.loc[filtered_indices].copy()\n",
    "df_reviews_with_sentiment_value_filtered.loc[:, 'label_idx'] = df_reviews_with_sentiment_value_filtered.label\\\n",
    "    .apply(lambda l: LABEL_MAPPING[l])\n",
    "\n",
    "df_reviews_with_sentiment_value_filtered[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "448c2d39-8b25-4499-ac35-cbde7d7a7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_LABEL_MAPPING = {\n",
    "    'training' : 1,\n",
    "    'testing': 2,\n",
    "    'validation': 3\n",
    "}\n",
    "\n",
    "MAX_SEQ_LENGTH = 40\n",
    "EMBEDDING_DIM = embedding.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590d3111-17dd-4c52-9f9f-883c6418d2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds):\n",
    "    filtered_indices = df_reviews_with_sentiment_value_filtered.splitset_label == SPLIT_LABEL_MAPPING[ds]\n",
    "    \n",
    "    reviews_in_ds = df_reviews_with_sentiment_value_filtered[filtered_indices]\n",
    "    \n",
    "    xd = np.zeros((len(reviews_in_ds), MAX_SEQ_LENGTH, EMBEDDING_DIM))\n",
    "    y = reviews_in_ds.label_idx.values.astype(int)\n",
    "    \n",
    "    reviews = []\n",
    "    for i, sostr in enumerate(reviews_in_ds.sostr.values):\n",
    "        sostr = sostr.lower()\n",
    "        review = []\n",
    "        for j, v in enumerate(sostr.split('|')[:MAX_SEQ_LENGTH]):\n",
    "            if v in encoder:\n",
    "                e_idx = encoder[v]\n",
    "            else:\n",
    "                e_idx = 0\n",
    "            \n",
    "            xd[i, j, :] = embedding[e_idx]\n",
    "            review.append(e_idx)\n",
    "        reviews.append(review)\n",
    "        \n",
    "\n",
    "    return dict(\n",
    "        x4d=np.expand_dims(xd, axis=1),\n",
    "        y=y,\n",
    "        encoded_reviews=reviews\n",
    "    )\n",
    "    \n",
    "\n",
    "DATASETS = dict()\n",
    "\n",
    "for ds in ['training', 'testing', 'validation']:\n",
    "    DATASETS[ds] = prepare_dataset(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9456b4-7bae-4114-a49b-20fc938469f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_reviews_with_sentiment_value), len(df_reviews_with_sentiment_value_filtered)\n",
    "sample_idx = 1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f652542-49d6-4807-9d99-2d6324fa1ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_shape, output_n, activation=None, dense_unit=256, dropout_rate=0.25):\n",
    "    if activation:\n",
    "        activation = \"relu\"\n",
    "\n",
    "    net = {}\n",
    "    net[\"in\"] = network_base.input_layer(shape=input_shape)\n",
    "    net[\"conv\"] = keras.layers.Conv2D(filters=100, kernel_size=(1,2), strides=(1, 1), padding='valid')(net[\"in\"])\n",
    "    net[\"pool\"] = keras.layers.MaxPooling2D(pool_size=(1, input_shape[2]-1), strides=(1,1))(net[\"conv\"])\n",
    "    net[\"out\"] = network_base.dense_layer(keras.layers.Flatten()(net[\"pool\"]), units=output_n, activation=activation)\n",
    "    net[\"sm_out\"] = network_base.softmax(net[\"out\"])\n",
    "\n",
    "\n",
    "    net.update({\n",
    "        \"input_shape\": input_shape,\n",
    "\n",
    "        \"output_n\": output_n,\n",
    "    })\n",
    "    return net\n",
    "\n",
    "net = build_network((None, 1, MAX_SEQ_LENGTH, EMBEDDING_DIM), NUM_CLASSES)\n",
    "model_without_softmax, model_with_softmax = Model(inputs=net['in'], outputs=net['out']), Model(inputs=net['in'], outputs=net['sm_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb7fb9a6-8033-467f-b747-277ea4e2201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    return keras.utils.to_categorical(y, NUM_CLASSES)\n",
    "\n",
    "def train_model(model,  batch_size=128, epochs=20):\n",
    "    \n",
    "    x_train = DATASETS['training']['x4d']\n",
    "    y_train = to_one_hot(DATASETS['training']['y'])\n",
    "    \n",
    "    x_test = DATASETS['testing']['x4d']\n",
    "    y_test = to_one_hot(DATASETS['testing']['y'])\n",
    "    \n",
    "    x_val = DATASETS['validation']['x4d']\n",
    "    y_val = to_one_hot(DATASETS['validation']['y'])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        shuffle=True\n",
    "                       )\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3cffab-5c0b-4cd8-93c2-877c7eb2c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model_with_softmax, batch_size=256, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce284c6e-28a7-4780-b0cb-720d644c7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_softmax.set_weights(model_with_softmax.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d210837-c801-41e1-9312-99332f251998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify methods that you would like to use to explain the model. \n",
    "# Please refer to iNNvestigate's documents for available methods.\n",
    "methods = ['gradient', 'lrp.z', 'lrp.alpha_2_beta_1', 'pattern.attribution']\n",
    "kwargs = [{}, {}, {}, {'pattern_type': 'relu'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ddd2fa-69d3-4841-8d6e-676af4cf05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an analyzer for each method\n",
    "analyzers = []\n",
    "\n",
    "for method, kws in zip(methods, kwargs):\n",
    "    analyzer = innvestigate.create_analyzer(method, model_without_softmax, **kws)\n",
    "    analyzer.fit(DATASETS['training']['x4d'], batch_size=256, verbose=1)\n",
    "    analyzers.append(analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a39d0-b12c-4ea8-924d-df552cad3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify indices of reviews that we want to investigate\n",
    "test_sample_indices = [97, 175, 1793, 1186, 354, 1043]\n",
    "\n",
    "test_sample_preds = [None]*len(test_sample_indices)\n",
    "\n",
    "# a variable to store analysis results.\n",
    "analysis = np.zeros([len(test_sample_indices), len(analyzers), 1, MAX_SEQ_LENGTH])\n",
    "\n",
    "for i, ridx in enumerate(test_sample_indices):\n",
    "\n",
    "    x, y = DATASETS['testing']['x4d'][ridx], DATASETS['testing']['y'][ridx]\n",
    "\n",
    "    t_start = time.time()\n",
    "    x = x.reshape((1, 1, MAX_SEQ_LENGTH, EMBEDDING_DIM))    \n",
    "\n",
    "    presm = model_without_softmax.predict_on_batch(x)[0] #forward pass without softmax\n",
    "    prob = model_with_softmax.predict_on_batch(x)[0] #forward pass with softmax\n",
    "    y_hat = prob.argmax()\n",
    "    test_sample_preds[i] = y_hat\n",
    "    \n",
    "    for aidx, analyzer in enumerate(analyzers):\n",
    "\n",
    "        a = np.squeeze(analyzer.analyze(x))\n",
    "        a = np.sum(a, axis=1)\n",
    "\n",
    "        analysis[i, aidx] = a\n",
    "    t_elapsed = time.time() - t_start\n",
    "    print('Review %d (%.4fs)'% (ridx, t_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e85d30-443d-42f5-b1a6-87c87c2b86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a utility method visualizing the relevance scores of each word to the network's prediction. \n",
    "# one might skip understanding the function, and see its output first.\n",
    "def plot_text_heatmap(words, scores, title=\"\", width=10, height=0.2, verbose=0, max_word_per_line=20):\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    \n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_title(title, loc='left')\n",
    "    tokens = words\n",
    "    if verbose > 0:\n",
    "        print('len words : %d | len scores : %d' % (len(words), len(scores)))\n",
    "\n",
    "    cmap = plt.cm.ScalarMappable(cmap=cm.bwr)\n",
    "    cmap.set_clim(0, 1)\n",
    "    \n",
    "    canvas = ax.figure.canvas\n",
    "    t = ax.transData\n",
    "\n",
    "    # normalize scores to the followings:\n",
    "    # - negative scores in [0, 0.5]\n",
    "    # - positive scores in (0.5, 1]\n",
    "    normalized_scores = 0.5 * scores / np.max(np.abs(scores)) + 0.5\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print('Raw score')\n",
    "        print(scores)\n",
    "        print('Normalized score')\n",
    "        print(normalized_scores)\n",
    "\n",
    "    # make sure the heatmap doesn't overlap with the title\n",
    "    loc_y = -0.2\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        *rgb, _ = cmap.to_rgba(normalized_scores[i], bytes=True)\n",
    "        color = '#%02x%02x%02x' % tuple(rgb)\n",
    "        \n",
    "        text = ax.text(0.0, loc_y, token,\n",
    "                       bbox={\n",
    "                           'facecolor': color,\n",
    "                           'pad': 5.0,\n",
    "                           'linewidth': 1,\n",
    "                           'boxstyle': 'round,pad=0.5'\n",
    "                       }, transform=t)\n",
    "\n",
    "        text.draw(canvas.get_renderer())\n",
    "        ex = text.get_window_extent()\n",
    "        \n",
    "        # create a new line if the line exceeds the length\n",
    "        if (i+1) % max_word_per_line == 0:\n",
    "            loc_y = loc_y -  2.5\n",
    "            t = ax.transData\n",
    "        else:\n",
    "            t = transforms.offset_copy(text._transform, x=ex.width+15, units='dots')\n",
    "\n",
    "    if verbose == 0:\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7eaa42-f598-421f-aa0e-275d5a033c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_heatmap(\n",
    "    \"I really love this movie but not in the beginning\".split(' '),\n",
    "    np.array([0.02, 0.2, 0.5, 0.1, 0.1, 0.1, -0.2, 0.05, 0.00, 0.08])\n",
    ")\n",
    "\n",
    "# \"love\" is shaded with strong red because its relevance score is rather high\n",
    "# \"not\" is highlighted in light blue because of its negative score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93d815-86c1-44a8-9049-f582d4cec0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(test_sample_indices):\n",
    "\n",
    "    words = [decoder[t] for t in list(DATASETS['testing']['encoded_reviews'][idx])]\n",
    "    \n",
    "    print('Review(id=%d): %s' % (idx, ' '.join(words)))\n",
    "    y_true = DATASETS['testing']['y'][idx]\n",
    "    y_pred = test_sample_preds[i]\n",
    "\n",
    "    print(\"Pred class : %s %s\" %\n",
    "          (LABEL_IDX_TO_NAME[y_pred], '✓' if y_pred == y_true else '✗ (%s)' % LABEL_IDX_TO_NAME[y_true])\n",
    "         )\n",
    "                                \n",
    "    for j, method in enumerate(methods):\n",
    "        plot_text_heatmap(words, analysis[i, j].reshape(-1), title='Method: %s' % method, verbose=0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea6406-0024-4e1e-ad92-a3f0b8b4dfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8a1ac-a12a-4d9c-ac4b-44bf3898326c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd55024-339a-4e84-bb50-fefe8ca12596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10faeee-b92b-455e-99b0-48f9de944557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
