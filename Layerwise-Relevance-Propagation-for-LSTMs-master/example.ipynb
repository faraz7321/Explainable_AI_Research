{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from lstm_network import LSTM_network\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# load the keras model and the word-2-vec dictionary. The keras model is trained for the sentiment analysis dataset\n",
    "# from stanford university and classifies movie reviews into five different categories.\n",
    "keras_model = load_model('model/sentiment_model.hdf5')\n",
    "w2v = pkl.load(open('model/token2vec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# create the lstm-lrp model\n",
    "n_hidden = 60\n",
    "embedding_dim = 60\n",
    "n_classes = 5\n",
    "weights = keras_model.get_weights()\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our keras model has no bias in the final dense layer. Therefore we add a bias of zero to the weights\n",
    "weights.append(np.zeros((n_classes,)))\n",
    "lrp_model = LSTM_network(n_hidden, embedding_dim, n_classes, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion is correct.\n"
     ]
    }
   ],
   "source": [
    "# test if the conversion was correct with an example sentence\n",
    "sentence = 'Neither funny nor suspenseful nor particularly well-drawn .'\n",
    "tokens = [s.lower() for s in sentence.split()]\n",
    "vecs = np.array([w2v[t] for t in tokens])\n",
    "y_keras = keras_model.predict(vecs[np.newaxis,:])\n",
    "y_lrpnet, _, _ = lrp_model.full_pass(vecs[np.newaxis,:])\n",
    "check = np.allclose(y_keras, y_lrpnet.numpy())\n",
    "print('Conversion is {}.'.format('correct' if check else 'wrong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 60)\n"
     ]
    }
   ],
   "source": [
    "# explain the classification\n",
    "eps = 1e-3\n",
    "bias_factor = 0.0\n",
    "# by setting y=None, the relevances will be calculated for the predicted class of the sample. We recommend this\n",
    "# usage, however, if you are interested in the relevances towards the 1st class, you could use y = np.array([1])\n",
    "explanation, Rest = lrp_model.lrp(vecs[np.newaxis,:], eps=eps, bias_factor=bias_factor)\n",
    "print(explanation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      neither:       1.86\n",
      "        funny:      -1.58\n",
      "          nor:       1.50\n",
      "  suspenseful:      -1.54\n",
      "          nor:       2.00\n",
      " particularly:      -0.04\n",
      "   well-drawn:      -0.06\n",
      "            .:      -0.12\n"
     ]
    }
   ],
   "source": [
    "# LRP assigns each dimension in the embedding vector a relevance value. To get relevances for each word we can\n",
    "# sum up these values\n",
    "word_relevances = tf.reduce_sum(explanation, axis=2)\n",
    "for word, relevance in zip(tokens, word_relevances[0]):\n",
    "    print('{0:>13}:   {1:8.2f}'.format(word, relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRP pass is correct.\n"
     ]
    }
   ],
   "source": [
    "# to check the correctness of the lrp pass we check if all relevance was preserved by using a bias factor of 1.0\n",
    "eps = 0.0\n",
    "bias_factor = 1.0\n",
    "explanation, Rest = lrp_model.lrp(vecs[np.newaxis,:], eps=eps, bias_factor=bias_factor)\n",
    "check = np.allclose(np.max(y_lrpnet.numpy()), np.sum(explanation)+np.sum(Rest))\n",
    "print('LRP pass is {}.'.format('correct' if check else 'wrong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10, 60)\n"
     ]
    }
   ],
   "source": [
    "# if all your input sequences have the same length you can process them batch-wise efficiently\n",
    "batch_size = 100\n",
    "length = 10\n",
    "some_random_data = tf.constant(np.random.randn(batch_size, length, embedding_dim))\n",
    "# explain 100 instances at once\n",
    "relevances, _ = lrp_model.lrp(some_random_data)\n",
    "print(relevances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
